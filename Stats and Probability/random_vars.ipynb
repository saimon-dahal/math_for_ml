{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321d6cae",
   "metadata": {},
   "source": [
    "## Random Variables (ML Intuition)\n",
    "\n",
    "### 1. What a Random Variable Is \n",
    "\n",
    "A **random variable** is **not** a variable in the usual programming sense.\n",
    "\n",
    "A random variable is a **deterministic computation applied to randomly sampled data**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Concrete ML Example\n",
    "\n",
    "Suppose you take a random sample from a dataset and apply a loss function to it.\n",
    "\n",
    "Ask:\n",
    "\n",
    "> Is the loss always the same number every time you run it?\n",
    "\n",
    "**No.**\n",
    "\n",
    "Why?\n",
    "\n",
    "* The loss function itself is fully deterministic.\n",
    "* If you give it the same `x` and `y`, it will always return the same value.\n",
    "\n",
    "The randomness comes from:\n",
    "\n",
    "* **which data point you happened to sample**.\n",
    "\n",
    "So the process is:\n",
    "\n",
    "> \"Pick a random data point, then compute a number from it.\"\n",
    "\n",
    "That resulting number is a **random variable**.\n",
    "\n",
    "Examples of random variables in ML:\n",
    "\n",
    "* loss\n",
    "* prediction\n",
    "* gradient norm\n",
    "* accuracy on a single sample\n",
    "\n",
    "Not because the formula is random —\n",
    "but because the **input is random**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loss as a Random Variable\n",
    "\n",
    "Take the learning objective:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta)\n",
    "=\n",
    "\\mathbb{E}_{(x,y)\\sim\\mathcal{D}}\n",
    "\\left[\n",
    "\\ell\\big(f_\\theta(x),\\, y\\big)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "* $\\ell(f_\\theta(x), y)$ measures **how bad the prediction was on one example**\n",
    "* Because $(x, y) \\sim \\mathcal{D}$ is sampled randomly, this loss is a **random variable**\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What the Expectation Means\n",
    "\n",
    "Expectation answers the question:\n",
    "\n",
    "> \"If I kept drawing data from the real world forever, what error would I usually make?\"\n",
    "\n",
    "So the full function answers:\n",
    "\n",
    "> **On average, how wrong is my model in the real world?**\n",
    "\n",
    "This is not an arbitrary function.\n",
    "It is a **quantification of failure**.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Why We Minimize Expected Loss\n",
    "\n",
    "Learning is fundamentally **choosing between models**.\n",
    "\n",
    "Every choice of parameters $\\theta$ gives:\n",
    "\n",
    "* a different function\n",
    "* a different behavior\n",
    "* a different expected error\n",
    "\n",
    "So the learning problem is:\n",
    "\n",
    "> **Which model makes the least mistake on average?**\n",
    "\n",
    "This is **decision-making under uncertainty**.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Should Expected Loss Be Zero?\n",
    "\n",
    "**No — not in general.**\n",
    "\n",
    "Expected loss can be zero *only if all of the following are true*:\n",
    "\n",
    "* the task is deterministic\n",
    "* labels contain no noise\n",
    "* the model class is expressive enough\n",
    "* the data distribution is perfectly learnable\n",
    "\n",
    "Examples where zero loss *is* achievable:\n",
    "\n",
    "* XOR with a correctly sized neural network\n",
    "* noiseless synthetic datasets\n",
    "\n",
    "In real ML problems:\n",
    "\n",
    "* labels are noisy\n",
    "* inputs are ambiguous\n",
    "* multiple outputs may be valid\n",
    "* the world itself is stochastic\n",
    "\n",
    "Therefore:\n",
    "\n",
    "> The minimum possible expected loss is **strictly greater than 0**.\n",
    "\n",
    "This lower bound is called **irreducible error**.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Loss Is Not Distance to Truth\n",
    "\n",
    "Loss is **not** a measure of absolute truth.\n",
    "\n",
    "Loss is a **scoring rule under uncertainty**.\n",
    "\n",
    "Example:\n",
    "\n",
    "* an image is blurry\n",
    "* the label is ambiguous\n",
    "* two humans disagree\n",
    "\n",
    "So what is the “correct” prediction?\n",
    "\n",
    "There isn’t one.\n",
    "\n",
    "The best model predicts a **distribution**, not a single point — and the loss reflects how well that distribution matches reality.\n",
    "\n",
    "So:\n",
    "\n",
    "* zero loss is not “truth”\n",
    "* zero loss is often **overconfidence**\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Why Different Losses Imply Different Optimal Predictions\n",
    "\n",
    "Loss functions are designed so that:\n",
    "\n",
    "> **Minimizing expected loss produces the best statistical decision**.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Squared error → predicts the **conditional mean**\n",
    "* Absolute error → predicts the **conditional median**\n",
    "* Cross-entropy → predicts **true class probabilities**\n",
    "\n",
    "None of these imply zero loss unless the world is trivial.\n",
    "\n",
    "They imply:\n",
    "\n",
    "> **Optimal behavior under uncertainty**\n",
    "\n",
    "That is the real goal of learning.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. What Counts as Evidence of Learning\n",
    "\n",
    "Evidence of learning is **not**:\n",
    "\n",
    "* training loss going to zero\n",
    "\n",
    "Evidence of learning **is**:\n",
    "\n",
    "* empirical loss ≈ expected loss (with high probability)\n",
    "* stability under resampling\n",
    "* low variance across batches\n",
    "\n",
    "> **We minimize expected loss to reach the best achievable tradeoff imposed by uncertainty in the data.**\n",
    "\n",
    "The minimum is not zero.\n",
    "The minimum is **whatever the world allows**.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Expected Loss vs Empirical Loss\n",
    "\n",
    "**Expected loss**:\n",
    "\n",
    "* average loss over the true data-generating distribution\n",
    "* includes all possible future datasets\n",
    "* what we actually care about\n",
    "\n",
    "**Empirical loss**:\n",
    "\n",
    "* average loss over a finite dataset\n",
    "* what we can measure\n",
    "\n",
    "With high probability, the difference between them is small —\n",
    "**but never guaranteed**.\n",
    "\n",
    "This statement is probabilistic, not absolute.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. When Empirical Loss Is a Bad Estimate\n",
    "\n",
    "Empirical loss can be misleading if:\n",
    "\n",
    "* dataset is small\n",
    "* data is biased\n",
    "* model is too flexible\n",
    "* labels are noisy in unrepresentative ways\n",
    "\n",
    "So the approximation does **not** hold by default.\n",
    "It holds only under assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "### 12. The Key Assumption \n",
    "\n",
    "The data we have must come from the **same distribution as future data**.\n",
    "\n",
    "This means:\n",
    "\n",
    "* independent samples\n",
    "* same underlying process\n",
    "* no hidden distribution shift\n",
    "\n",
    "This is why **distribution shift breaks models**.\n",
    "\n",
    "---\n",
    "\n",
    "### 13. Why Empirical Loss Converges to Expected Loss\n",
    "\n",
    "Each per-sample loss is a **random variable**.\n",
    "\n",
    "Empirical loss is just the **average** of these random variables.\n",
    "\n",
    "Probability theory tells us:\n",
    "\n",
    "> The average of i.i.d. random variables converges to their expectation.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. What Controls the Gap Between Empirical and Expected Loss\n",
    "\n",
    "The gap depends on:\n",
    "\n",
    "1. **Dataset size**\n",
    "2. **Model complexity**\n",
    "   Simpler hypothesis class → smaller gap\n",
    "3. **Loss variance**\n",
    "   Noisy loss → slower convergence\n",
    "4. **How hard you searched**\n",
    "   More tuning → more optimism bias\n",
    "\n",
    "---\n",
    "\n",
    "### 15. Final Mental Model\n",
    "\n",
    "* Loss, gradients, predictions → **random variables**\n",
    "* Expected loss → true objective\n",
    "* Empirical loss → noisy estimate\n",
    "* Learning → minimizing expected loss under uncertainty\n",
    "\n",
    "This is why probability theory is not optional in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05de67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
